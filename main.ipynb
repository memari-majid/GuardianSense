{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Medical Emergency Detection Agent \n",
    "\n",
    "## 1. Introduction to AI Agents\n",
    "\n",
    "An **AI Agent** is a system that perceives its environment through sensors, processes the data, and takes actions autonomously to achieve specific goals. In this project, the AI agent aims to detect medical emergencies by analyzing multiple types of data:\n",
    "\n",
    "- **Text**: Patient statements or medical notes.\n",
    "- **Images**: Facial expressions indicating pain or distress.\n",
    "- **Audio**: Speech content that may suggest an emergency.\n",
    "- **Video**: Movements indicating falls or accidents.\n",
    "- **Physiological Data**: Vital signs like heart rate and blood pressure.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Autonomous Agent**: Operates independently without continuous human guidance.\n",
    "- **Multimodal Agent**: Processes and integrates multiple types of data.\n",
    "- **Intelligent Agent**: Makes decisions based on AI algorithms and models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Overview of the Code\n",
    "\n",
    "The code is structured to perform the following tasks:\n",
    "\n",
    "1. **Initialize AI Models**: Sets up models for text, speech, image, and video analysis.\n",
    "2. **Data Acquisition**: Simulates or accepts input data from various modalities.\n",
    "3. **Data Processing**: Processes each data type using appropriate AI models.\n",
    "4. **Data Fusion**: Combines insights from all modalities to make an informed decision.\n",
    "5. **Decision-Making**: Determines if a medical emergency is occurring.\n",
    "6. **User Interface**: Provides a Streamlit-based GUI for user interaction.\n",
    "7. **Alert Mechanism**: Triggers visual and auditory alerts if an emergency is detected.\n",
    "8. **Interactive Q&A**: Allows users to ask questions, with answers generated by the LLaMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Import all necessary libraries required for the application, including standard libraries, machine learning models, and Streamlit for the web interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import random          # For generating random synthetic data and simulating processes\n",
    "import time            # For time-related functions, if needed in the future\n",
    "import cv2             # OpenCV library for video processing\n",
    "import numpy as np      # For numerical operations, especially with arrays\n",
    "import torch           # PyTorch library for deep learning models\n",
    "import librosa         # For audio processing and feature extraction\n",
    "import warnings        # To manage warning messages\n",
    "\n",
    "# Suppress all warnings to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import necessary libraries for Ollama integration\n",
    "import requests        # To make HTTP requests to the Ollama API\n",
    "import json            # To handle JSON data\n",
    "import os              # For interacting with the operating system (e.g., file handling)\n",
    "import base64          # For encoding binary data to base64 (useful for embedding media)\n",
    "\n",
    "# Import models for image and speech processing\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor  # Pre-trained models for speech-to-text\n",
    "from PIL import Image                                     # For image processing\n",
    "\n",
    "# Import Streamlit for creating the web-based GUI\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CSS for UI Enhancement\n",
    "\n",
    "Define custom CSS styles to enhance the appearance of the Streamlit web interface, including font styles, button colors, and emergency alert animations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_css():\n",
    "    \"\"\"\n",
    "    Adds custom CSS styles to the Streamlit app to enhance the UI appearance.\n",
    "    \"\"\"\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        /* Set the default font for the body */\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        }\n",
    "        /* Set the background color for the main content area */\n",
    "        .main {\n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        /* Style for Streamlit buttons */\n",
    "        .stButton > button {\n",
    "            background-color: #4CAF50; /* Green background */\n",
    "            color: white;              /* White text */\n",
    "        }\n",
    "        /* Style for the emergency alert banner */\n",
    "        .emergency-alert {\n",
    "            background-color: red;     /* Red background to indicate urgency */\n",
    "            color: white;              /* White text for contrast */\n",
    "            font-size: 24px;           /* Larger font size */\n",
    "            text-align: center;        /* Centered text */\n",
    "            padding: 20px;             /* Padding around the content */\n",
    "            border-radius: 10px;       /* Rounded corners */\n",
    "            animation: blink 1s infinite; /* Blinking animation to attract attention */\n",
    "        }\n",
    "        /* Keyframes for the blinking animation */\n",
    "        @keyframes blink {\n",
    "            0% {opacity: 1;}\n",
    "            50% {opacity: 0.5;}\n",
    "            100% {opacity: 1;}\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True  # Allow raw HTML for custom styling\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Audio Files\n",
    "\n",
    "Utility function to read an audio file and encode it to base64 format. This is useful for embedding audio directly into HTML for playback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_base64(file_path):\n",
    "    \"\"\"\n",
    "    Reads an audio file from the given file path and encodes it to base64.\n",
    "    This is useful for embedding audio directly into HTML.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the audio file to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: The base64-encoded string of the audio file, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()  # Read the binary data of the audio file\n",
    "        data_base64 = base64.b64encode(data).decode('utf-8')  # Encode to base64 and decode to string\n",
    "        return data_base64\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding audio file: {e}\")  # Log the error\n",
    "        return None  # Return None if encoding fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing LLaMA via Ollama\n",
    "\n",
    "Function to initialize the LLaMA model using the Ollama API. This setup allows sending prompts to the model and receiving generated responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llama_via_ollama():\n",
    "    \"\"\"\n",
    "    Initializes the LLaMA model via the Ollama API.\n",
    "    This function sets up a closure that can be used to send prompts to the LLaMA model\n",
    "    and receive generated responses.\n",
    "\n",
    "    Returns:\n",
    "    - function: A function that takes a prompt string and returns the model's response.\n",
    "    \"\"\"\n",
    "    base_url = \"http://localhost:11434\"  # Base URL for the Ollama API, typically running locally\n",
    "\n",
    "    def llama_model(prompt):\n",
    "        \"\"\"\n",
    "        Sends a prompt to the LLaMA model via Ollama and retrieves the generated response.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt (str): The input text prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the LLaMA model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            headers = {\"Content-Type\": \"application/json\"}  # Set the content type for the request\n",
    "            data = {\n",
    "                \"model\": \"llama3.2\",  # Specify the LLaMA 3.2 RAG model\n",
    "                \"prompt\": prompt      # The prompt to send to the model\n",
    "            }\n",
    "            # Send a POST request to the Ollama API's generate endpoint with streaming enabled\n",
    "            response = requests.post(\n",
    "                f\"{base_url}/api/generate\",\n",
    "                headers=headers,\n",
    "                data=json.dumps(data),\n",
    "                stream=True  # Enable streaming to receive the response incrementally\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                output = \"\"  # Initialize an empty string to accumulate the response\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        decoded_line = line.decode('utf-8')  # Decode the byte stream to string\n",
    "                        try:\n",
    "                            json_data = json.loads(decoded_line)  # Parse the JSON data\n",
    "                            output += json_data.get('response', '')  # Append the response part\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue  # If JSON is invalid, skip to the next line\n",
    "                return output.strip()  # Return the accumulated response without leading/trailing whitespace\n",
    "            else:\n",
    "                # Log errors if the response status is not OK\n",
    "                print(f\"Error communicating with Ollama: {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "                return \"\"  # Return an empty string on error\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle exceptions related to the HTTP request\n",
    "            print(f\"Error communicating with Ollama: {e}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected exceptions\n",
    "            print(f\"Exception in llama_model: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    return llama_model  # Return the closure function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data\n",
    "\n",
    "Function to generate synthetic physiological data that mimics real-world data from devices like an Apple Watch. This includes metrics such as heart rate, oxygen saturation, blood pressure, steps, calories burned, and sleep hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data():\n",
    "    \"\"\"\n",
    "    Generates synthetic physiological data to mimic data that might be collected from a device like an Apple Watch.\n",
    "    This data includes heart rate, oxygen saturation, blood pressure, steps, calories burned, and sleep hours.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the synthetic physiological data.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'heart_rate': random.randint(50, 150),                # Heart rate in beats per minute (bpm)\n",
    "        'oxygen_saturation': random.uniform(85, 100),         # Oxygen saturation in percentage (%)\n",
    "        'blood_pressure_systolic': random.randint(90, 160),    # Systolic blood pressure in mmHg\n",
    "        'blood_pressure_diastolic': random.randint(60, 100),   # Diastolic blood pressure in mmHg\n",
    "        'steps': random.randint(0, 10000),                     # Number of steps taken\n",
    "        'calories_burned': random.uniform(0, 500),             # Calories burned in kilocalories (kcal)\n",
    "        'sleep_hours': random.uniform(0, 12),                  # Sleep duration in hours\n",
    "    }\n",
    "    return data  # Return the generated data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Video\n",
    "\n",
    "Function to process a video file and detect features relevant to medical emergencies, such as falls. Currently, this function simulates video processing by randomly determining if a fall is detected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    \"\"\"\n",
    "    Processes a video file to extract features relevant to medical emergency detection, such as fall detection.\n",
    "    Currently, this function simulates video processing by randomly determining if a fall is detected.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path (str): The file path to the video to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if a fall is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the video file exists\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Error: Video file {video_path} does not exist.\")\n",
    "            return False  # Return False if the video file is missing\n",
    "\n",
    "        # For simplicity, simulate video processing with random fall detection\n",
    "        fall_detected = random.choice([True, False])\n",
    "        return fall_detected  # Return the simulated result\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions during video processing\n",
    "        print(f\"Error in video processing: {e}\")\n",
    "        return False  # Default to False on error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Image\n",
    "\n",
    "Function to analyze an image and detect facial expressions, simulating emotion detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Analyzes an image to detect facial expressions, simulating emotion detection.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): The file path to the image to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: The detected dominant emotion or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the image file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Error: Image file {image_path} does not exist.\")\n",
    "            return None  # Return None if the image file is missing\n",
    "\n",
    "        # Load the image to ensure it's readable\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        # Simulate emotion detection by randomly selecting an emotion\n",
    "        emotions = ['happy', 'sad', 'angry', 'surprised', 'neutral']\n",
    "        dominant_emotion = random.choice(emotions)\n",
    "        return dominant_emotion  # Return the simulated emotion\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions during image processing\n",
    "        print(f\"Error in image processing: {e}\")\n",
    "        return None  # Return None on error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Speech\n",
    "\n",
    "Function to convert speech in an audio file to text using a pre-trained Wav2Vec2 model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guardian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
