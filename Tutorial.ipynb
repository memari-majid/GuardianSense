{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztdRtqil_JG9"
      },
      "source": [
        "# Multimodal Medical Emergency Detection Agent\n",
        "\n",
        "## 1. Introduction to AI Agents\n",
        "\n",
        "An **AI Agent** is a system that perceives its environment through sensors, processes the data, and takes actions autonomously to achieve specific goals. In this project, the AI agent aims to detect medical emergencies by analyzing multiple types of data:\n",
        "\n",
        "- **Text**: Patient statements or medical notes.\n",
        "- **Images**: Facial expressions indicating pain or distress.\n",
        "- **Audio**: Speech content that may suggest an emergency.\n",
        "- **Video**: Movements indicating falls or accidents.\n",
        "- **Physiological Data**: Vital signs like heart rate and blood pressure.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Autonomous Agent**: Operates independently without continuous human guidance.\n",
        "- **Multimodal Agent**: Processes and integrates multiple types of data.\n",
        "- **Intelligent Agent**: Makes decisions based on AI algorithms and models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiDTgO-k_JHA"
      },
      "source": [
        "\n",
        "## 2. Overview of the Code\n",
        "\n",
        "The code is structured to perform the following tasks:\n",
        "\n",
        "1. **Initialize AI Models**: Sets up models for text, speech, image, and video analysis.\n",
        "2. **Data Acquisition**: Simulates or accepts input data from various modalities.\n",
        "3. **Data Processing**: Processes each data type using appropriate AI models.\n",
        "4. **Data Fusion**: Combines insights from all modalities to make an informed decision.\n",
        "5. **Decision-Making**: Determines if a medical emergency is occurring.\n",
        "6. **User Interface**: Provides a Streamlit-based GUI for user interaction.\n",
        "7. **Alert Mechanism**: Triggers visual and auditory alerts if an emergency is detected.\n",
        "8. **Interactive Q&A**: Allows users to ask questions, with answers generated by the LLaMA model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFzOXn97_JHA"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "Import all necessary libraries required for the application, including standard libraries, machine learning models, and Streamlit for the web interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCNjMfw-_JHA"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import random          # For generating random synthetic data and simulating processes\n",
        "import time            # For time-related functions, if needed in the future\n",
        "import cv2             # OpenCV library for video processing\n",
        "import numpy as np      # For numerical operations, especially with arrays\n",
        "import torch           # PyTorch library for deep learning models\n",
        "import librosa         # For audio processing and feature extraction\n",
        "import warnings        # To manage warning messages\n",
        "\n",
        "# Suppress all warnings to keep the output clean\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import necessary libraries for Ollama integration\n",
        "import requests        # To make HTTP requests to the Ollama API\n",
        "import json            # To handle JSON data\n",
        "import os              # For interacting with the operating system (e.g., file handling)\n",
        "import base64          # For encoding binary data to base64 (useful for embedding media)\n",
        "\n",
        "# Import models for image and speech processing\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor  # Pre-trained models for speech-to-text\n",
        "from PIL import Image                                     # For image processing\n",
        "\n",
        "# Import Streamlit for creating the web-based GUI\n",
        "import streamlit as st\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnt0NcVN_JHC"
      },
      "source": [
        "## Custom CSS for UI Enhancement\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function enhances the visual appearance of your Streamlit application by injecting custom CSS (Cascading Style Sheets) rules. CSS is a language used to style HTML elements, controlling aspects like fonts, colors, layout, and more.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **`st.markdown`:** It utilizes the `st.markdown` function from Streamlit to render HTML content containing the CSS styles.\n",
        "2. **`<style>` tag:** The CSS rules are enclosed within `<style>` tags, signaling to the browser that this is CSS code.\n",
        "3. **CSS rules:**\n",
        "    -  `body`: Targets the entire body of the webpage, setting the default font family to 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif.\n",
        "    - `.main`: This likely targets the main content area of your Streamlit app, giving it a light gray background color (`#f5f5f5`).\n",
        "    - `.stButton > button`: Selects buttons within Streamlit's button containers and styles them with a green background (`#4CAF50`) and white text color (`white`).\n",
        "    - `.emergency-alert`: Defines styles for a potential emergency alert element, setting a red background, white text, larger font size, and adding a blinking animation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPddSS2W_JHC"
      },
      "outputs": [],
      "source": [
        "def add_custom_css():\n",
        "    \"\"\"\n",
        "    Adds custom CSS styles to the Streamlit app to enhance the UI appearance.\n",
        "    \"\"\"\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        /* Set the default font for the body */\n",
        "        body {\n",
        "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "        }\n",
        "        /* Set the background color for the main content area */\n",
        "        .main {\n",
        "            background-color: #f5f5f5;\n",
        "        }\n",
        "        /* Style for Streamlit buttons */\n",
        "        .stButton > button {\n",
        "            background-color: #4CAF50; /* Green background */\n",
        "            color: white;              /* White text */\n",
        "        }\n",
        "        /* Style for the emergency alert banner */\n",
        "        .emergency-alert {\n",
        "            background-color: red;     /* Red background to indicate urgency */\n",
        "            color: white;              /* White text for contrast */\n",
        "            font-size: 24px;           /* Larger font size */\n",
        "            text-align: center;        /* Centered text */\n",
        "            padding: 20px;             /* Padding around the content */\n",
        "            border-radius: 10px;       /* Rounded corners */\n",
        "            animation: blink 1s infinite; /* Blinking animation to attract attention */\n",
        "        }\n",
        "        /* Keyframes for the blinking animation */\n",
        "        @keyframes blink {\n",
        "            0% {opacity: 1;}\n",
        "            50% {opacity: 0.5;}\n",
        "            100% {opacity: 1;}\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True  # Allow raw HTML for custom styling\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4hGTOKo_JHC"
      },
      "source": [
        "### Encoding Audio Files\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function takes an audio file as input and converts it into a base64-encoded string. This encoding is useful for embedding audio data directly into HTML for playback within a web page or application.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **File Reading:** It opens the audio file specified by `file_path` in binary read mode (`'rb'`).\n",
        "2. **Binary Data Reading:**  It reads the entire content of the audio file into a variable called `data`, representing the raw binary data of the audio.\n",
        "3. **Base64 Encoding:** It utilizes the `base64.b64encode` function to encode the binary audio data (`data`) into base64 format. Base64 encoding is a way to represent binary data using only printable ASCII characters.\n",
        "4. **Decoding to String:** The encoded data, which is in bytes format, is then decoded into a UTF-8 string using `.decode('utf-8')`. This string format is suitable for embedding within HTML.\n",
        "5. **Return Value:** The function returns the base64-encoded string representation of the audio file. If any error occurs during the process, it returns `None`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge9NYSr-_JHC"
      },
      "outputs": [],
      "source": [
        "def get_audio_base64(file_path):\n",
        "    \"\"\"\n",
        "    Reads an audio file from the given file path and encodes it to base64.\n",
        "    This is useful for embedding audio directly into HTML.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): The path to the audio file to be encoded.\n",
        "\n",
        "    Returns:\n",
        "    - str or None: The base64-encoded string of the audio file, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            data = f.read()  # Read the binary data of the audio file\n",
        "        data_base64 = base64.b64encode(data).decode('utf-8')  # Encode to base64 and decode to string\n",
        "        return data_base64\n",
        "    except Exception as e:\n",
        "        print(f\"Error encoding audio file: {e}\")  # Log the error\n",
        "        return None  # Return None if encoding fails\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUC3Nnrk_JHD"
      },
      "source": [
        "### Initializing LLM\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function initializes and sets up access to a LLaMA language model through the Ollama API. It essentially creates a function (`llama_model`) that can be used to send prompts to the LLaMA model and receive generated text responses.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **Base URL:** It defines the `base_url` for the Ollama API, which is assumed to be running locally on port 11434.\n",
        "2. **`llama_model` Closure:** It defines a nested function called `llama_model`, which is a closure. This inner function takes a `prompt` as input and communicates with the Ollama API to get a response from the LLaMA model.\n",
        "3. **Request Preparation:** Within `llama_model`, it sets up the necessary headers and data for the API request, including specifying the LLaMA model to use (`llama3.2`) and the input `prompt`.\n",
        "4. **API Interaction:** It uses the `requests` library to send a POST request to the Ollama API's `/api/generate` endpoint. The `stream=True` parameter enables receiving the response incrementally.\n",
        "5. **Response Handling:** It iterates through the response stream, decodes the data, and extracts the generated text from the JSON response.\n",
        "6. **Error Handling:** It includes error handling for potential issues like invalid JSON, network errors, or API errors.\n",
        "7. **Return Value:** The `initialize_llama_via_ollama` function returns the `llama_model` closure function. This returned function can then be used to interact with the LLaMA model by passing prompts and receiving generated responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95CmRibY_JHD"
      },
      "outputs": [],
      "source": [
        "def initialize_llama_via_ollama():\n",
        "    \"\"\"\n",
        "    Initializes the LLaMA model via the Ollama API.\n",
        "    This function sets up a closure that can be used to send prompts to the LLaMA model\n",
        "    and receive generated responses.\n",
        "\n",
        "    Returns:\n",
        "    - function: A function that takes a prompt string and returns the model's response.\n",
        "    \"\"\"\n",
        "    base_url = \"http://localhost:11434\"  # Base URL for the Ollama API, typically running locally\n",
        "\n",
        "    def llama_model(prompt):\n",
        "        \"\"\"\n",
        "        Sends a prompt to the LLaMA model via Ollama and retrieves the generated response.\n",
        "\n",
        "        Parameters:\n",
        "        - prompt (str): The input text prompt to send to the model.\n",
        "\n",
        "        Returns:\n",
        "        - str: The generated response from the LLaMA model.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            headers = {\"Content-Type\": \"application/json\"}  # Set the content type for the request\n",
        "            data = {\n",
        "                \"model\": \"llama3.2\",  # Specify the LLaMA 3.2 RAG model\n",
        "                \"prompt\": prompt      # The prompt to send to the model\n",
        "            }\n",
        "            # Send a POST request to the Ollama API's generate endpoint with streaming enabled\n",
        "            response = requests.post(\n",
        "                f\"{base_url}/api/generate\",\n",
        "                headers=headers,\n",
        "                data=json.dumps(data),\n",
        "                stream=True  # Enable streaming to receive the response incrementally\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                output = \"\"  # Initialize an empty string to accumulate the response\n",
        "                for line in response.iter_lines():\n",
        "                    if line:\n",
        "                        decoded_line = line.decode('utf-8')  # Decode the byte stream to string\n",
        "                        try:\n",
        "                            json_data = json.loads(decoded_line)  # Parse the JSON data\n",
        "                            output += json_data.get('response', '')  # Append the response part\n",
        "                        except json.JSONDecodeError:\n",
        "                            continue  # If JSON is invalid, skip to the next line\n",
        "                return output.strip()  # Return the accumulated response without leading/trailing whitespace\n",
        "            else:\n",
        "                # Log errors if the response status is not OK\n",
        "                print(f\"Error communicating with Ollama: {response.status_code}\")\n",
        "                print(f\"Response: {response.text}\")\n",
        "                return \"\"  # Return an empty string on error\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # Handle exceptions related to the HTTP request\n",
        "            print(f\"Error communicating with Ollama: {e}\")\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "            # Handle any other unexpected exceptions\n",
        "            print(f\"Exception in llama_model: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    return llama_model  # Return the closure function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ4vITJJ_JHD"
      },
      "source": [
        "### Generating Synthetic Data\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function simulates the generation of physiological data that might be collected from a wearable device like an Apple Watch. It creates a dictionary containing synthetic values for various health metrics.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **Data Dictionary:** It initializes an empty dictionary called `data` to store the generated values.\n",
        "2. **Data Generation:** It uses the `random` module to generate random values for each physiological metric:\n",
        "    - `heart_rate`: Random integer between 50 and 150, representing heart rate in beats per minute (bpm).\n",
        "    - `oxygen_saturation`: Random float between 85 and 100, representing oxygen saturation in percentage (%).\n",
        "    - `blood_pressure_systolic`: Random integer between 90 and 160, representing systolic blood pressure in mmHg.\n",
        "    - `blood_pressure_diastolic`: Random integer between 60 and 100, representing diastolic blood pressure in mmHg.\n",
        "    - `steps`: Random integer between 0 and 10000, representing the number of steps taken.\n",
        "    - `calories_burned`: Random float between 0 and 500, representing calories burned in kilocalories (kcal).\n",
        "    - `sleep_hours`: Random float between 0 and 12, representing sleep duration in hours.\n",
        "3. **Return Value:** The function returns the `data` dictionary containing the generated synthetic physiological data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlqvUTxV_JHD"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generates synthetic physiological data to mimic data that might be collected from a device like an Apple Watch.\n",
        "    This data includes heart rate, oxygen saturation, blood pressure, steps, calories burned, and sleep hours.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing the synthetic physiological data.\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'heart_rate': random.randint(50, 150),                # Heart rate in beats per minute (bpm)\n",
        "        'oxygen_saturation': random.uniform(85, 100),         # Oxygen saturation in percentage (%)\n",
        "        'blood_pressure_systolic': random.randint(90, 160),    # Systolic blood pressure in mmHg\n",
        "        'blood_pressure_diastolic': random.randint(60, 100),   # Diastolic blood pressure in mmHg\n",
        "        'steps': random.randint(0, 10000),                     # Number of steps taken\n",
        "        'calories_burned': random.uniform(0, 500),             # Calories burned in kilocalories (kcal)\n",
        "        'sleep_hours': random.uniform(0, 12),                  # Sleep duration in hours\n",
        "    }\n",
        "    return data  # Return the generated data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_R_0a9Z_JHE"
      },
      "source": [
        "### Video Processing\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function is intended to process a video file and identify features relevant to medical emergency detection, specifically focusing on fall detection. Currently, it acts as a placeholder by simulating video processing and randomly determining whether a fall has been detected.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **File Existence Check:** It first checks if the video file specified by `video_path` exists using `os.path.exists`. If the file is not found, it prints an error message and returns `False`.\n",
        "2. **Simulated Fall Detection:** If the video file is found, it proceeds to simulate fall detection. For demonstration purposes, it randomly selects either `True` (fall detected) or `False` (fall not detected) using `random.choice`.\n",
        "3. **Return Value:** The function returns the simulated result, either `True` or `False`, indicating whether a fall was detected (simulated) or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TJziNGQ_JHE"
      },
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "    \"\"\"\n",
        "    Processes a video file to extract features relevant to medical emergency detection, such as fall detection.\n",
        "    Currently, this function simulates video processing by randomly determining if a fall is detected.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (str): The file path to the video to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if a fall is detected, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the video file exists\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"Error: Video file {video_path} does not exist.\")\n",
        "            return False  # Return False if the video file is missing\n",
        "\n",
        "        # For simplicity, simulate video processing with random fall detection\n",
        "        fall_detected = random.choice([True, False])\n",
        "        return fall_detected  # Return the simulated result\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions during video processing\n",
        "        print(f\"Error in video processing: {e}\")\n",
        "        return False  # Default to False on error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3LDKE3Y_JHE"
      },
      "source": [
        "### Image Processing\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function is designed to analyze an image and simulate the detection of facial expressions, essentially mimicking emotion detection capabilities.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **File Existence Check:** It begins by checking if the image file specified by `image_path` exists using `os.path.exists`. If the file is not found, it prints an error message and returns `None`.\n",
        "2. **Image Loading:** If the image file exists, it attempts to load the image using `Image.open` from the `PIL` library to ensure it is readable.\n",
        "3. **Simulated Emotion Detection:** To simulate emotion detection, the function randomly selects an emotion from a predefined list (`emotions`). This list typically includes emotions like 'happy', 'sad', 'angry', 'surprised', and 'neutral'.\n",
        "4. **Return Value:** The function returns the randomly selected emotion as a string. If any error occurs during the process, it returns `None`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4lV_Gu3_JHE"
      },
      "outputs": [],
      "source": [
        "def process_image(image_path):\n",
        "    \"\"\"\n",
        "    Analyzes an image to detect facial expressions, simulating emotion detection.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path (str): The file path to the image to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - str or None: The detected dominant emotion or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the image file exists\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Error: Image file {image_path} does not exist.\")\n",
        "            return None  # Return None if the image file is missing\n",
        "\n",
        "        # Load the image to ensure it's readable\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Simulate emotion detection by randomly selecting an emotion\n",
        "        emotions = ['happy', 'sad', 'angry', 'surprised', 'neutral']\n",
        "        dominant_emotion = random.choice(emotions)\n",
        "        return dominant_emotion  # Return the simulated emotion\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions during image processing\n",
        "        print(f\"Error in image processing: {e}\")\n",
        "        return None  # Return None on error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGy1Bo7F_JHE"
      },
      "source": [
        "### Speech Recognition\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function converts speech present in an audio file into written text using a pre-trained Wav2Vec2 model from the Hugging Face Transformers library.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **File Existence Check:** It first checks if the audio file specified by `audio_path` exists using `os.path.exists`. If the file is not found, it prints an error message and returns `None`.\n",
        "2. **Model Loading:** It loads the pre-trained Wav2Vec2 processor and model using `Wav2Vec2Processor.from_pretrained` and `Wav2Vec2ForCTC.from_pretrained`, respectively. These components are essential for speech-to-text conversion.\n",
        "3. **Audio Processing:** It reads the audio file using `librosa.load`, performs pre-emphasis if needed, and converts the audio data into a format suitable for the Wav2Vec2 model.\n",
        "4. **Speech-to-Text Conversion:** It uses the loaded processor and model to convert the audio data into text. This involves extracting features, generating logits, and decoding them into a text sequence.\n",
        "5. **Text Processing:** It converts the transcribed text to lowercase and returns it. If any errors occur during the process, it returns `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXA4lAZJPDuo"
      },
      "outputs": [],
      "source": [
        "def process_speech(audio_path):\n",
        "    \"\"\"\n",
        "    Converts speech in an audio file to text using a pre-trained Wav2Vec2 model.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_path (str): The file path to the audio file to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - str or None: The transcribed text in lowercase or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the audio file exists\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"Error: Audio file {audio_path} does not exist.\")\n",
        "            return None  # Return None if the audio file is missing\n",
        "\n",
        "        # Load pre-trained Wav2Vec2 processor and model for speech-to-text\n",
        "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "        # Load the audio file using librosa, resampling to 16kHz\n",
        "        speech, rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        # Check if the audio file is empty or unreadable\n",
        "        if len(speech) == 0:\n",
        "            print(\"Error: Audio file is empty or cannot be read.\")\n",
        "            return None  # Return None if audio data is invalid\n",
        "\n",
        "        # Tokenize the audio input for the model\n",
        "        input_values = processor(speech, return_tensors='pt', padding='longest').input_values\n",
        "\n",
        "        # Perform inference to get logits from the model\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "        # Get the predicted token IDs by taking the argmax of logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Decode the predicted token IDs to get the transcribed text\n",
        "        transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "        return transcription.lower()  # Return the transcription in lowercase\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions during speech processing\n",
        "        print(f\"Error in speech processing: {e}\")\n",
        "        return None  # Return None on error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iReoUSUqPDfY"
      },
      "source": [
        "### Text Processing with LLM\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function utilizes the LLaMA language model, accessed through the Ollama API, to analyze input text for indications of a medical emergency.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **Prompt Construction:** It creates a prompt string that instructs the LLaMA model to analyze the provided `input_text` for signs of a medical emergency. The prompt is formatted to provide context and guide the model's response.\n",
        "2. **LLaMA Model Interaction:** It calls the `llama_model` function (which was initialized earlier using `initialize_llama_via_ollama`) with the constructed prompt. This sends the prompt to the LLaMA model via the Ollama API.\n",
        "3. **Response Retrieval:** It receives the response generated by the LLaMA model, which contains the analysis of the input text.\n",
        "4. **Return Value:** The function returns the LLaMA model's response as a string. This response typically includes an assessment of whether the input text suggests a medical emergency and may provide further insights or explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohOlxBUCPsMd"
      },
      "outputs": [],
      "source": [
        "def process_text_llama(input_text, llama_model):\n",
        "    \"\"\"\n",
        "    Analyzes input text using the LLaMA model via Ollama to determine signs of medical emergency.\n",
        "\n",
        "    Parameters:\n",
        "    - input_text (str): The text input to be analyzed.\n",
        "    - llama_model (function): The LLaMA model function initialized via Ollama.\n",
        "\n",
        "    Returns:\n",
        "    - str: The analysis result from the LLaMA model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a prompt that instructs the model to analyze the patient statement\n",
        "        prompt = f\"Analyze the following patient statement for signs of medical emergency:\\n\\n\\\"{input_text}\\\"\\n\\nIs there an emergency? Provide a brief explanation.\"\n",
        "\n",
        "        # Get the response from the LLaMA model\n",
        "        response = llama_model(prompt)\n",
        "\n",
        "        if response:\n",
        "            return response.strip()  # Return the trimmed response if available\n",
        "        else:\n",
        "            return \"No response from LLaMA model.\"  # Default message if no response\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions during text processing\n",
        "        print(f\"Error in process_text_llama: {e}\")\n",
        "        return \"Error analyzing text.\"  # Return an error message\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E4VhriiQJJz"
      },
      "source": [
        "### Data Fusion and Decision-Making\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function plays a crucial role in the medical emergency detection system by combining data from multiple sources to make a comprehensive assessment of whether a medical emergency has occurred. It integrates information from physiological sensors, video analysis, image processing, and text/speech analysis.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **Data Input:** It receives data from various sources as input parameters:\n",
        "    - `physio_data`: A dictionary containing physiological data like heart rate, blood pressure, etc.\n",
        "    - `fall_detected`: A boolean value indicating whether a fall was detected in the video.\n",
        "    - `emotion`: A string representing the detected emotion from facial expression analysis.\n",
        "    - `speech_text`: The transcribed text from speech input.\n",
        "    - `speech_analysis`: The analysis of the speech text performed by the LLaMA model.\n",
        "    - `text_analysis`: The analysis of any additional text input, also performed by the LLaMA model.\n",
        "2. **Data Fusion Logic:** The core of this function is its logic to combine the different data points and determine the likelihood of a medical emergency. This logic can be complex and may involve rules, thresholds, or machine learning models to assess the overall situation.\n",
        "3. **Emergency Determination:** Based on the combined data analysis, the function decides whether a medical emergency is likely to have occurred.\n",
        "4. **Return Value:** It returns a boolean value (`True` if an emergency is detected, `False` otherwise) and a string containing a summary or explanation of the decision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8QPY1BT1QIzL"
      },
      "outputs": [],
      "source": [
        "def data_fusion(physio_data, fall_detected, emotion, speech_text, speech_analysis, text_analysis):\n",
        "    \"\"\"\n",
        "    Fuses data from various sources (physiological, video, image, audio, text) to make a decision\n",
        "    about whether a medical emergency has occurred.\n",
        "\n",
        "    Parameters:\n",
        "    - physio_data (dict): Physiological data metrics.\n",
        "    - fall_detected (bool): Whether a fall was detected in the video.\n",
        "    - emotion (str): Detected emotion from the image.\n",
        "    - speech_text (str): Transcribed speech text.\n",
        "    - speech_analysis (str): Analysis of the speech text by LLaMA.\n",
        "    - text_analysis (str): Analysis of the JSON text input by LLaMA.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing alerts (list), decision message (str), warnings (list), and emergency_detected (bool).\n",
        "    \"\"\"\n",
        "    alerts = []          # List to store alert messages\n",
        "    warnings_list = []   # List to store warning messages\n",
        "\n",
        "    # -------------------------------\n",
        "    # Check Physiological Data\n",
        "    # -------------------------------\n",
        "    if physio_data:\n",
        "        # Check for abnormal heart rate\n",
        "        if physio_data['heart_rate'] > 120 or physio_data['heart_rate'] < 50:\n",
        "            alerts.append('Abnormal heart rate detected.')\n",
        "        # Check for low oxygen saturation\n",
        "        if physio_data['oxygen_saturation'] < 90.0:\n",
        "            alerts.append('Low oxygen saturation detected.')\n",
        "        # Check for abnormal blood pressure\n",
        "        if physio_data['blood_pressure_systolic'] > 140 or physio_data['blood_pressure_systolic'] < 90:\n",
        "            alerts.append('Abnormal blood pressure detected.')\n",
        "    else:\n",
        "        # Warn if physiological data is missing or invalid\n",
        "        warnings_list.append(\"Physiological data is missing or invalid.\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Check Fall Detection\n",
        "    # -------------------------------\n",
        "    if fall_detected is True:\n",
        "        alerts.append('Fall detected.')\n",
        "    elif fall_detected is False:\n",
        "        pass  # No fall detected; no action needed\n",
        "    else:\n",
        "        # Warn if fall detection data is missing or invalid\n",
        "        warnings_list.append(\"Fall detection data is missing or invalid.\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Check Emotion Detection\n",
        "    # -------------------------------\n",
        "    if emotion in ['sad', 'angry']:\n",
        "        alerts.append(f'Negative emotion detected: {emotion}.')\n",
        "    elif emotion is None:\n",
        "        # Warn if emotion data is missing or invalid\n",
        "        warnings_list.append(\"Emotion data is missing or invalid.\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Check Speech Content Using LLaMA\n",
        "    # -------------------------------\n",
        "    if speech_text and speech_analysis:\n",
        "        # If the word 'emergency' is detected in the speech analysis, trigger an alert\n",
        "        if 'emergency' in speech_analysis.lower():\n",
        "            alerts.append('Emergency detected in speech.')\n",
        "    else:\n",
        "        # Warn if speech analysis is missing or invalid\n",
        "        warnings_list.append(\"Speech analysis is missing or invalid.\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Check Text Input Analysis\n",
        "    # -------------------------------\n",
        "    if text_analysis:\n",
        "        # If the word 'emergency' is detected in the text analysis, trigger an alert\n",
        "        if 'emergency' in text_analysis.lower():\n",
        "            alerts.append('Emergency detected in text input.')\n",
        "    else:\n",
        "        # Warn if text analysis is missing or invalid\n",
        "        warnings_list.append(\"Text analysis is missing or invalid.\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Decision-Making Using a Simple Rule-Based Model\n",
        "    # -------------------------------\n",
        "    if len(alerts) >= 2:\n",
        "        # If there are two or more alerts, consider it a medical emergency\n",
        "        decision = 'Medical emergency detected! Triggering alarm.'\n",
        "        emergency_detected = True\n",
        "    else:\n",
        "        # Otherwise, no emergency is detected\n",
        "        decision = 'No emergency detected.'\n",
        "        emergency_detected = False\n",
        "\n",
        "    return alerts, decision, warnings_list, emergency_detected  # Return all relevant information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLw5j730Q6aT"
      },
      "source": [
        "### Agent Function\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function serves as the main entry point and orchestrator for the entire Medical Emergency Detection Agent application. It handles various tasks, including setting up the user interface using Streamlit, managing file uploads, processing data from different modalities (text, image, audio, video, physiological), performing analysis using AI models, and presenting the results to the user.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. **UI Initialization and Styling:**\n",
        "    - `add_custom_css()`: Calls the `add_custom_css` function to apply custom CSS styles, enhancing the visual appearance of the Streamlit web interface.\n",
        "    - `st.image()`: Displays an image of an \"AI doctor\" at the top of the app using `st.image`, creating a visual representation of the system's purpose.\n",
        "    - `st.title()`: Sets the main title of the application to \"Multimodal Medical Emergency Detection Agent\" using `st.title`, providing a clear heading.\n",
        "\n",
        "2. **Data Input and Processing:**\n",
        "    - **File Uploads:** Utilizes Streamlit's file upload components (`st.file_uploader`) to allow the user to upload files for text, image, audio, and video data.\n",
        "    - **Physiological Data:** Generates synthetic physiological data using `generate_synthetic_data` or accepts data from real sensors if available.\n",
        "    - **Data Preprocessing:** Calls the appropriate processing functions (`process_text_llama`, `process_image`, `process_speech`, `process_video`) to analyze the uploaded/provided data and extract relevant features or insights.\n",
        "    - **LLaMA Model Initialization:** Initializes the LLaMA model for text analysis through the Ollama API using `initialize_llama_via_ollama`.\n",
        "\n",
        "3. **Data Fusion and Decision-Making:**\n",
        "    - `data_fusion()`: Calls the `data_fusion` function to integrate the results from different data sources, taking into account physiological data, fall detection, emotion detection, speech analysis, and text analysis.\n",
        "    - **Emergency Determination:**  `data_fusion` function assesses the combined data and determines whether a medical emergency is likely to have occurred based on predefined rules or thresholds.\n",
        "\n",
        "4. **Results Display and Alerting:**\n",
        "    - **Emergency Alert:** If a medical emergency is detected, the application triggers an alert, displaying a prominent emergency banner with a blinking animation and potentially sounding an audio alert.\n",
        "    - **Result Summary:** Presents a summary of the analysis results to the user, including the detected emotion, transcribed speech text, and insights from the LLaMA model's analysis.\n",
        "    - **Interactive Q&A:** Provides a section for the user to ask questions about their condition or the analysis results. The LLaMA model is used to generate answers, offering a more interactive and informative experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aREg1NWOQcvc"
      },
      "outputs": [],
      "source": [
        "def medical_emergency_agent():\n",
        "    \"\"\"\n",
        "    The main function that runs the Streamlit web application for the Medical Emergency Detection Agent.\n",
        "    It handles the UI, file uploads, data processing, analysis, and displays results to the user.\n",
        "    \"\"\"\n",
        "    add_custom_css()  # Apply custom CSS styles to enhance the UI\n",
        "\n",
        "    # Display the AI doctor image at the top of the app\n",
        "    st.image('AI_doctor.png', use_column_width=True)\n",
        "\n",
        "    # Set the main title of the application\n",
        "    st.title(\"Multimodal Medical Emergency Detection Agent\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Sidebar for Navigation and Exit Button\n",
        "    # -------------------------------\n",
        "    st.sidebar.title(\"Navigation\")  # Title for the sidebar\n",
        "    # Dropdown menu for selecting the app mode\n",
        "    app_mode = st.sidebar.selectbox(\"Choose the app mode\",\n",
        "                                    [\"Home\", \"Upload Files\", \"View Results\"])\n",
        "\n",
        "    # Exit Button in Sidebar\n",
        "    if st.sidebar.button(\"Exit Application\"):\n",
        "        st.sidebar.write(\"Exiting the application...\")  # Inform the user\n",
        "        st.stop()  # Stop the Streamlit app\n",
        "\n",
        "    # -------------------------------\n",
        "    # Home Page\n",
        "    # -------------------------------\n",
        "    if app_mode == \"Home\":\n",
        "        st.header(\"Welcome!\")  # Header for the Home page\n",
        "        st.write(\"\"\"\n",
        "            This application detects medical emergencies by analyzing multimodal inputs, including physiological data, video, images, audio, and text.\n",
        "            Please navigate to **Upload Files** to provide input data, or proceed to **View Results** to see the analysis.\n",
        "        \"\"\")  # Description of the app's functionality\n",
        "\n",
        "    # -------------------------------\n",
        "    # Upload Files Page\n",
        "    # -------------------------------\n",
        "    elif app_mode == \"Upload Files\":\n",
        "        st.header(\"Upload Files for Analysis\")  # Header for the Upload Files page\n",
        "\n",
        "        try:\n",
        "            # Initialize LLaMA via Ollama\n",
        "            llama_model = initialize_llama_via_ollama()\n",
        "\n",
        "            # Generate synthetic physiological data\n",
        "            physio_data = generate_synthetic_data()\n",
        "            st.subheader(\"Physiological Data\")  # Subheader for physiological data\n",
        "            st.write(physio_data)  # Display the synthetic physiological data\n",
        "\n",
        "            # -------------------------------\n",
        "            # File Uploads Section\n",
        "            # -------------------------------\n",
        "            st.subheader(\"File Uploads\")  # Subheader for file uploads\n",
        "\n",
        "            # -------------------------------\n",
        "            # Video File Upload\n",
        "            # -------------------------------\n",
        "            video_file = st.file_uploader(\"Upload a video file\", type=['mp4', 'avi', 'mov'])  # File uploader for video\n",
        "            if video_file is not None:\n",
        "                # If a video file is uploaded, save it to a temporary directory\n",
        "                video_path = os.path.join('temp_video', video_file.name)\n",
        "                os.makedirs('temp_video', exist_ok=True)  # Create the directory if it doesn't exist\n",
        "                with open(video_path, 'wb') as f:\n",
        "                    f.write(video_file.getbuffer())  # Write the uploaded file's bytes to disk\n",
        "                fall_detected = process_video(video_path)  # Process the uploaded video\n",
        "            else:\n",
        "                # If no video is uploaded, use a default video file\n",
        "                st.info(\"No video file uploaded. Using default 'fall.mp4'.\")\n",
        "                video_path = 'fall.mp4'\n",
        "                if not os.path.exists(video_path):\n",
        "                    st.error(f\"Default video file '{video_path}' not found.\")  # Error if default video is missing\n",
        "                    fall_detected = None\n",
        "                else:\n",
        "                    fall_detected = process_video(video_path)  # Process the default video\n",
        "\n",
        "            # -------------------------------\n",
        "            # Image File Upload\n",
        "            # -------------------------------\n",
        "            image_file = st.file_uploader(\"Upload an image file\", type=['jpg', 'jpeg', 'png'])  # File uploader for image\n",
        "            if image_file is not None:\n",
        "                # If an image file is uploaded, save it to a temporary directory\n",
        "                image_path = os.path.join('temp_image', image_file.name)\n",
        "                os.makedirs('temp_image', exist_ok=True)  # Create the directory if it doesn't exist\n",
        "                with open(image_path, 'wb') as f:\n",
        "                    f.write(image_file.getbuffer())  # Write the uploaded file's bytes to disk\n",
        "                emotion = process_image(image_path)  # Process the uploaded image\n",
        "            else:\n",
        "                # If no image is uploaded, use a default image file\n",
        "                st.info(\"No image file uploaded. Using default 'pain.jpg'.\")\n",
        "                image_path = 'pain.jpg'\n",
        "                if not os.path.exists(image_path):\n",
        "                    st.error(f\"Default image file '{image_path}' not found.\")  # Error if default image is missing\n",
        "                    emotion = None\n",
        "                else:\n",
        "                    emotion = process_image(image_path)  # Process the default image\n",
        "\n",
        "            # -------------------------------\n",
        "            # Audio File Upload\n",
        "            # -------------------------------\n",
        "            audio_file = st.file_uploader(\"Upload an audio file\", type=['wav', 'mp3'])  # File uploader for audio\n",
        "            if audio_file is not None:\n",
        "                # If an audio file is uploaded, save it to a temporary directory\n",
        "                audio_path = os.path.join('temp_audio', audio_file.name)\n",
        "                os.makedirs('temp_audio', exist_ok=True)  # Create the directory if it doesn't exist\n",
        "                with open(audio_path, 'wb') as f:\n",
        "                    f.write(audio_file.getbuffer())  # Write the uploaded file's bytes to disk\n",
        "                speech_text = process_speech(audio_path)  # Process the uploaded audio\n",
        "            else:\n",
        "                # If no audio is uploaded, set speech_text to None\n",
        "                st.info(\"No audio file uploaded.\")\n",
        "                speech_text = None\n",
        "\n",
        "            # -------------------------------\n",
        "            # JSON File Upload\n",
        "            # -------------------------------\n",
        "            json_file = st.file_uploader(\"Upload a JSON file\", type=['json'])  # File uploader for JSON\n",
        "            if json_file is not None:\n",
        "                try:\n",
        "                    # Attempt to load the uploaded JSON file\n",
        "                    json_data = json.load(json_file)\n",
        "                    input_text = json_data.get('text', \"No text found in JSON.\")  # Extract 'text' field\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error reading uploaded JSON file: {e}\")  # Error if JSON is invalid\n",
        "                    input_text = \"Error reading JSON file.\"\n",
        "            else:\n",
        "                # If no JSON is uploaded, use a default JSON file\n",
        "                st.info(\"No JSON file uploaded. Using default 'database.json'.\")\n",
        "                if not os.path.exists('database.json'):\n",
        "                    st.error(\"Default JSON file 'database.json' not found.\")  # Error if default JSON is missing\n",
        "                    input_text = \"No input provided.\"\n",
        "                else:\n",
        "                    try:\n",
        "                        with open('database.json', 'r') as f:\n",
        "                            json_data = json.load(f)  # Load the default JSON file\n",
        "                            input_text = json_data.get('text', \"No text found in JSON.\")  # Extract 'text' field\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error reading default JSON file: {e}\")  # Error if default JSON is invalid\n",
        "                        input_text = \"Error reading JSON file.\"\n",
        "\n",
        "            # -------------------------------\n",
        "            # Save Session State\n",
        "            # -------------------------------\n",
        "            # Store all relevant data in Streamlit's session state for later use\n",
        "            st.session_state['physio_data'] = physio_data\n",
        "            st.session_state['fall_detected'] = fall_detected\n",
        "            st.session_state['emotion'] = emotion\n",
        "            st.session_state['speech_text'] = speech_text\n",
        "            st.session_state['input_text'] = input_text\n",
        "            st.session_state['llama_model'] = llama_model\n",
        "\n",
        "            # Inform the user that files have been uploaded and processed successfully\n",
        "            st.success(\"Files uploaded and processed successfully! Navigate to 'View Results' to see the analysis.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any unexpected errors during the upload and processing phase\n",
        "            st.error(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # View Results Page\n",
        "    # -------------------------------\n",
        "    elif app_mode == \"View Results\":\n",
        "        st.header(\"Analysis Results\")  # Header for the View Results page\n",
        "\n",
        "        # Check if data has been uploaded; if not, prompt the user to upload files first\n",
        "        if 'physio_data' not in st.session_state:\n",
        "            st.warning(\"No data found. Please upload files first.\")\n",
        "            return  # Exit the function if no data is present\n",
        "\n",
        "        # Retrieve all necessary data from the session state\n",
        "        physio_data = st.session_state.get('physio_data', {})\n",
        "        fall_detected = st.session_state.get('fall_detected', None)\n",
        "        emotion = st.session_state.get('emotion', None)\n",
        "        speech_text = st.session_state.get('speech_text', None)\n",
        "        input_text = st.session_state.get('input_text', \"\")\n",
        "        llama_model = st.session_state.get('llama_model', None)\n",
        "\n",
        "        # -------------------------------\n",
        "        # Display Uploaded and Processed Data\n",
        "        # -------------------------------\n",
        "        st.subheader(\"Physiological Data\")  # Subheader for physiological data\n",
        "        st.write(physio_data)  # Display the physiological data\n",
        "\n",
        "        st.subheader(\"Uploaded Data Analysis\")  # Subheader for uploaded data analysis\n",
        "        st.write(f\"**Fall Detected:** {fall_detected}\")           # Display fall detection result\n",
        "        st.write(f\"**Detected Emotion:** {emotion}\")             # Display detected emotion\n",
        "        st.write(f\"**Transcribed Speech:** {speech_text}\")       # Display transcribed speech text\n",
        "        st.write(f\"**Text from JSON:** {input_text}\")            # Display text from JSON input\n",
        "\n",
        "        # -------------------------------\n",
        "        # Analyze Speech Text Using LLaMA\n",
        "        # -------------------------------\n",
        "        with st.spinner('Analyzing speech text with AI Doctor...'):  # Show a spinner during processing\n",
        "            if speech_text and llama_model:\n",
        "                # Analyze the transcribed speech text using LLaMA\n",
        "                speech_analysis = process_text_llama(speech_text, llama_model)\n",
        "            else:\n",
        "                speech_analysis = None  # Set to None if no speech text or model is available\n",
        "\n",
        "        # -------------------------------\n",
        "        # Analyze Text Input Using LLaMA\n",
        "        # -------------------------------\n",
        "        with st.spinner('Analyzing text input with AI Doctor...'):  # Show a spinner during processing\n",
        "            if input_text and llama_model:\n",
        "                # Analyze the JSON text input using LLaMA\n",
        "                text_analysis = process_text_llama(input_text, llama_model)\n",
        "            else:\n",
        "                text_analysis = None  # Set to None if no input text or model is available\n",
        "\n",
        "        # -------------------------------\n",
        "        # Display LLaMA Analysis\n",
        "        # -------------------------------\n",
        "        st.subheader(\"Analysis by AI Doctor\")  # Subheader for AI Doctor's analysis\n",
        "        st.info(\"The data has been analyzed by the AI Doctor.\")  # Inform the user\n",
        "\n",
        "        st.subheader(\"LLaMA Analysis\")  # Subheader for LLaMA-specific analysis\n",
        "        if speech_analysis:\n",
        "            st.write(f\"**Speech Analysis:**\\n{speech_analysis}\")  # Display speech analysis result\n",
        "        else:\n",
        "            st.write(\"No speech input or analysis.\")  # Inform if speech analysis is unavailable\n",
        "\n",
        "        if text_analysis:\n",
        "            st.write(f\"**Text Analysis:**\\n{text_analysis}\")  # Display text analysis result\n",
        "        else:\n",
        "            st.write(\"No text analysis available.\")  # Inform if text analysis is unavailable\n",
        "\n",
        "        # -------------------------------\n",
        "        # Perform Data Fusion and Make Decision\n",
        "        # -------------------------------\n",
        "        alerts, decision, warnings_list, emergency_detected = data_fusion(\n",
        "            physio_data, fall_detected, emotion, speech_text, speech_analysis, text_analysis\n",
        "        )\n",
        "\n",
        "        # -------------------------------\n",
        "        # Display Alerts and Decision\n",
        "        # -------------------------------\n",
        "        st.subheader(\"Alerts\")  # Subheader for alerts\n",
        "        if alerts:\n",
        "            for alert in alerts:\n",
        "                st.error(f\"- {alert}\")  # Display each alert as an error message\n",
        "        else:\n",
        "            st.write(\"No alerts.\")  # Inform if there are no alerts\n",
        "\n",
        "        st.subheader(\"Decision\")  # Subheader for the decision message\n",
        "        if emergency_detected:\n",
        "            # If an emergency is detected, display a graphical alert and play an alarm sound\n",
        "\n",
        "            # Display a styled emergency alert banner with an alarm icon\n",
        "            st.markdown(\n",
        "                \"\"\"\n",
        "                <div class=\"emergency-alert\">\n",
        "                    <img src=\"https://img.icons8.com/emoji/48/alarm-clock-emoji.png\" width=\"50\" style=\"vertical-align: middle;\"/>\n",
        "                    <span style=\"vertical-align: middle;\">Medical Emergency Detected! Triggering Alarm!</span>\n",
        "                </div>\n",
        "                \"\"\",\n",
        "                unsafe_allow_html=True  # Allow raw HTML for styling\n",
        "            )\n",
        "            st.warning(\"Emergency detected! Please respond immediately.\")  # Warning message to the user\n",
        "\n",
        "            # Play alarm sound automatically by embedding the audio\n",
        "            audio_base64 = get_audio_base64('alarm_sound.mp3')  # Encode the alarm sound to base64\n",
        "            if audio_base64:\n",
        "                st.markdown(\n",
        "                    f\"\"\"\n",
        "                    <audio autoplay>\n",
        "                        <source src=\"data:audio/mp3;base64,{audio_base64}\" type=\"audio/mp3\">\n",
        "                    </audio>\n",
        "                    \"\"\",\n",
        "                    unsafe_allow_html=True  # Embed the audio player with autoplay\n",
        "                )\n",
        "            else:\n",
        "                st.error(\"Alarm sound file not found or could not be read.\")  # Error if audio file is missing\n",
        "\n",
        "        else:\n",
        "            st.success(decision)  # Display the decision message as a success message\n",
        "\n",
        "        # -------------------------------\n",
        "        # Display Warnings, If Any\n",
        "        # -------------------------------\n",
        "        if warnings_list:\n",
        "            st.subheader(\"Warnings\")  # Subheader for warnings\n",
        "            for warning in warnings_list:\n",
        "                st.warning(warning)  # Display each warning as a warning message\n",
        "\n",
        "        # -------------------------------\n",
        "        # Note to User About Autoplay Policies\n",
        "        # -------------------------------\n",
        "        if emergency_detected:\n",
        "            st.info(\"Note: If you do not hear the alarm sound, your browser may have blocked autoplay. Please adjust your browser settings to allow autoplay of audio.\")\n",
        "\n",
        "        # -------------------------------\n",
        "        # New Section: Ask Questions with Fact-Checking\n",
        "        # -------------------------------\n",
        "\n",
        "        st.header(\"Ask Questions about the Data\")  # Header for the Q&A section\n",
        "\n",
        "        if llama_model:\n",
        "            # Prepare a summary of all data to provide context for the AI Doctor\n",
        "            data_summary = f\"\"\"\n",
        "            Physiological Data:\n",
        "            {physio_data}\n",
        "\n",
        "            Fall Detected: {fall_detected}\n",
        "            Detected Emotion: {emotion}\n",
        "            Transcribed Speech: {speech_text}\n",
        "            Text from JSON: {input_text}\n",
        "            Alerts: {alerts}\n",
        "            \"\"\"\n",
        "\n",
        "            st.write(\"You can ask questions about the uploaded data. The AI Doctor will answer your questions based on the data provided.\")  # Instructions for the user\n",
        "\n",
        "            # Text input for the user to enter a question\n",
        "            question = st.text_input(\"Enter your question:\")\n",
        "            if question:\n",
        "                with st.spinner('Generating answer...'):  # Show a spinner while generating the answer\n",
        "                    # Create a prompt that includes the data summary and the user's question\n",
        "                    prompt = f\"\"\"\n",
        "                    Based on the following data:\n",
        "\n",
        "                    {data_summary}\n",
        "\n",
        "                    Answer the following question:\n",
        "\n",
        "                    {question}\n",
        "                    \"\"\"\n",
        "                    answer = llama_model(prompt)  # Get the answer from the LLaMA model\n",
        "                    if answer:\n",
        "                        st.subheader(\"Answer:\")  # Subheader for the answer\n",
        "                        st.write(answer)  # Display the answer\n",
        "\n",
        "                        # -------------------------------\n",
        "                        # Fact-Check the Answer Using the Medical Model\n",
        "                        # -------------------------------\n",
        "                        with st.spinner('Fact-checking the answer with the medical model...'):  # Show a spinner during fact-checking\n",
        "                            # Create a prompt to fact-check the provided answer\n",
        "                            fact_check_prompt = f\"\"\"\n",
        "                            Based on the following data:\n",
        "\n",
        "                            {data_summary}\n",
        "\n",
        "                            The following answer was provided to the question \"{question}\":\n",
        "\n",
        "                            {answer}\n",
        "\n",
        "                            Is this answer correct based on the data provided? Provide a brief explanation and correct any inaccuracies.\n",
        "                            \"\"\"\n",
        "                            fact_check_result = llama_model(fact_check_prompt)  # Get the fact-check result from the model\n",
        "                            if fact_check_result:\n",
        "                                st.subheader(\"Fact-Check Result:\")  # Subheader for the fact-check result\n",
        "                                st.write(fact_check_result)  # Display the fact-check result\n",
        "                            else:\n",
        "                                st.error(\"No response from the medical model during fact-checking.\")  # Error if fact-checking fails\n",
        "                    else:\n",
        "                        st.error(\"No response from the AI Doctor.\")  # Error if no answer is generated\n",
        "        else:\n",
        "            st.error(\"LLaMA model is not initialized.\")  # Error if the LLaMA model is unavailable\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "guardian",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
